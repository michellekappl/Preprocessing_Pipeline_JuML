var documenterSearchIndex = {"docs":
[{"location":"getting_started/#Installation-in-REPL","page":"Getting Started","title":"Installation in REPL","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Clone the repository and include the package in your Julia environment:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"git clone git@github.com:michellekappl/Preprocessing_Pipeline_JuML.git\ncd path/to/dir/Preprocessing_Pipeline_JuML","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Then, activate the package in Julia:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.activate(\".\") \nPkg.instantiate()\nusing Preprocessing_Pipeline_JuML","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This will install the required dependencies and make the package available in your Julia REPL.","category":"page"},{"location":"getting_started/#Quickstart","page":"Getting Started","title":"Quickstart","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"1. Load the package:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Preprocessing_Pipeline_JuML","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"2. Prepare a test corpus: Define a set of noisy text samples for preprocessing:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"test_corpus = [\n   \"Hello <b>world</b>! Visit http://example.com.\",\n   \"Email me: test@example.com or call +123-456-7890.\",\n   \"Today is 12/25/2024, time now: 10:30AM.\",\n   \"My file is at C:\\\\Users\\\\JohnDoe\\\\Documents\\\\file.txt.\",\n   \"Check this out: www.awesome-website.org/about-us.html!\",\n   \"#JuliaLang is great. Follow us @JuliaNLP.\"\n]","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"3. Build your pipeline:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"pipe = NlpPipe(test_corpus) |> remove_noise |> tokenize |> one_hot_encoding","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"4. Inspect the pipeline outputs:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Get the tokenized representation:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"@info pipe.tokens # contains documents split into single tokens","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"5. View the vectors and vocabulary generated during vectorization:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"@info pipe.tokens # contains the tokens from before, but as numeric vectors\n@info pipe.vocabulary","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<!â€“ ## License","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This project is licensed under the MIT License. See the LICENSE file for details. â€“>","category":"page"},{"location":"getting_started/#Run-the-tests","page":"Getting Started","title":"Run the tests","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To run the tests, open a Julia REPL, activate the project, and use the ] key to enter the package management console. Then, execute test to run the test suite. The output should look like this:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(@v1.6) pkg> test\nTesting:                    |   52     52  0.6s\n  NlpPipe Tests             |   10     10  0.1s\n  TokenizedNlpPipe Tests    |    8      8  0.1s\n  Remove Stop Words Tests   |    3      3  0.1s\n  Expand Contractions Tests |    3      3  0.1s\n  Mask Numbers Tests        |    6      6  0.0s\n  Remove Noise Tests        |    6      6  0.0s\n  Standardize Text Tests    |    2      2  0.0s\n  OneHotEncoding Tests      |    2      2  0.1s\n  Bag of Words Tests        |    4      4  0.0s\n  BagOfNGrams Tests         |    8      8  0.1s\n     Testing Preprocessing_Pipeline_JuML tests passed","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Questions will be abbreviated with Q and answers with A.","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Q: \"is there a regex for finding digits within a string, independent of length?\"\nA: \"Yes, there is a regex for finding digits within a string regardless of their length. The pattern is: \\d+","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Explanation:     - \\d matches any single digit (0-9).     - + matches one or more occurrences of the preceding element (in this case, digits). This means \\d+ will match any sequence of consecutive digits in a string, regardless of how long the sequence is.\"","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Q: \"can you list all word contractions that are possible in english language\"\nA: \"Here is a comprehensive list of common contractions in English, categorized for easier understanding. While this list may not be exhaustive, it includes nearly all widely used contractions:","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Contractions with \"not\" Contraction\tExpansion aren't\tare not can't\tcannot couldn't\tcould not didn't\tdid not doesn't\tdoes not don't\tdo not hadn't\thad not hasn't\thas not haven't\thave not isn't\tis not mightn't\tmight not mustn't\tmust not needn't\tneed not shouldn't\tshould not wasn't\twas not weren't\twere not won't\twill not wouldn't\twould not Contractions with \"is/has\" Contraction\tExpansion he's\the is / he has she's\tshe is / she has it's\tit is / it has that's\tthat is / that has there's\tthere is / there has where's\twhere is / where has who's\twho is / who has what's\twhat is / what has how's\thow is / how has Contractions with \"will\" Contraction\tExpansion I'll\tI will you'll\tyou will he'll\the will she'll\tshe will it'll\tit will we'll\twe will they'll\tthey will Contractions with \"would/had\" Contraction\tExpansion I'd\tI would / I had you'd\tyou would / you had he'd\the would / he had she'd\tshe would / she had it'd\tit would / it had we'd\twe would / we had they'd\tthey would / they had Contractions with \"have\" Contraction\tExpansion I've\tI have you've\tyou have we've\twe have they've\tthey have who've\twho have would've\twould have could've\tcould have should've\tshould have might've\tmight have Contractions with \"am\" Contraction\tExpansion I'm\tI am Contractions with \"are\" Contraction\tExpansion you're\tyou are we're\twe are they're\tthey are Contractions with \"let\" Contraction\tExpansion let's\tlet us Contractions with \"there\" Contraction\tExpansion there's\tthere is / there has Contractions with \"here\" Contraction\tExpansion here's\there is Miscellaneous Contractions Contraction\tExpansion ain't\tam not / is not / are not / has not / have not o'clock\tof the clock y'all\tyou all ma'am\tmadam","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"This list includes the most commonly used contractions in English. Note that some contractions (like \"ain't\" or \"y'all\") are informal or dialectal. Others, like \"o'clock\", are more specific in usage.\"","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Q: \"is there a regex that can find word contractions like we collected above\"\nA: \"Yes, you can use a regex to match English word contractions like those collected above. To do so, the regex should be designed to handle patterns involving an apostrophe (') followed by specific suffixes or entire contracted words.  Here is a general regex that matches the common word contractions listed: \\b(?:[a-zA-Z]+n't|[a-zA-Z]+'(?:ll|re|ve|d|s|m|t))\\b\"\nQ: how do i set optional keyword arguments in julia in the argument of a function?\nA: In Julia, you can define optional keyword arguments by including them after a semicolon (;) in the function signature. Keyword arguments can have default values, making them optional.","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Hereâ€™s the general syntax:","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"function myfunction(positionalarg1, positionalarg2; keywordarg1=defaultvalue1, keywordarg2=defaultvalue2)     # Function body     println(\"Positional Args: positionalarg1, positional_arg2\")     println(\"Keyword Args: keyword_arg1, keyword_arg2\") end","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Key Points:","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Positional Arguments come before the semicolon (;).\nKeyword Arguments are declared after the semicolon and are accessed using name=value when calling the function.\nKeyword arguments can have default values, making them optional.","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Q: \"can you provide an example call for stemming  for a stemming algorithm in julia please?\"\nA: \"Certainly! Here's an example of how to use a stemming algorithm in Julia with the SnowballStemmer.jl package. I'll show you a typical call for stemming words using this package.","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Example: Using Snowball Stemmer for English Text","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Install the SnowballStemmer Package (if you haven't done so already):","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"using Pkg Pkg.add(\"SnowballStemmer\")","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Load the Package and Stem Words:","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"using SnowballStemmer","category":"page"},{"location":"llm_questions/#Create-an-English-stemmer","page":"Create an English stemmer","title":"Create an English stemmer","text":"","category":"section"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"stemmer = Stemmer(\"English\")","category":"page"},{"location":"llm_questions/#Sample-input","page":"Create an English stemmer","title":"Sample input","text":"","category":"section"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"words = [\"running\", \"jumps\", \"happily\", \"cats\"]","category":"page"},{"location":"llm_questions/#Apply-stemming-to-each-word-in-the-vector","page":"Create an English stemmer","title":"Apply stemming to each word in the vector","text":"","category":"section"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"stemmed_words = [stemmer(word) for word in words]","category":"page"},{"location":"llm_questions/#Display-the-stemmed-words","page":"Create an English stemmer","title":"Display the stemmed words","text":"","category":"section"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"println(stemmed_words)  # Output: [\"run\", \"jump\", \"happili\", \"cat\"]","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Explanation:","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Stemmer(\"English\"): This creates an instance of the English language stemmer.\nstemmer(word): Applies the stemming algorithm to a single word.\nList comprehension [stemmer(word) for word in words]: This applies stemming to each word in the words vector.\nOutput: The result is a vector of stemmed words, e.g., [\"run\", \"jump\", \"happili\", \"cat\"].","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Example Output:","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"[\"run\", \"jump\", \"happili\", \"cat\"]","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"\"running\" becomes \"run\".\n\"jumps\" becomes \"jump\".\n\"happily\" becomes \"happili\" (stemming doesn't always give perfect results for all words).\n\"cats\" becomes \"cat\".","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Summary:","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Creating the stemmer: stemmer = Stemmer(\"English\").\nStemming words: Use stemmer(word) for each word.\nVectorized application: Use list comprehension to stem multiple words.","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"You can replace \"English\" with other languages supported by the SnowballStemmer package (like \"German\", \"Spanish\", etc.), depending on your needs.","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Let me know if you need more examples or adjustments! ðŸ˜Š\"","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Q: what do i call the tag if i replace a number with <NUM> in nl\nA: The process of replacing numbers with <NUM> in natural language (NL) processing is typically referred to as \"number masking\" or \"number normalization\". The tag <NUM> itself is often called a placeholder tag, masking token, or entity tag.","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Common terminology: Number masking: The act of replacing numbers in text with a generic placeholder (e.g., <NUM>). Placeholder tag: Refers to the specific token used, such as <NUM>, <NUMBER>, or any other chosen label. Normalization: A broader term encompassing various methods of transforming text into a standardized form, including masking numbers. Would you like help implementing this in a specific scenario?","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Q: what other masks exist except for <NUM>\nA: In Natural Language Processing (NLP), masking involves replacing specific types of tokens with placeholders to generalize or anonymize the text. Apart from <NUM>, here are other common masks:","category":"page"},{"location":"llm_questions/","page":"Create an English stemmer","title":"Create an English stemmer","text":"Entity Masks <DATE>: For dates and times (e.g., \"January 1, 2025\" â†’ <DATE>). <TIME>: For time expressions (e.g., \"3:30 PM\" â†’ <TIME>). <URL>: For URLs or web links (e.g., \"https://example.com\" â†’ <URL>). <EMAIL>: For email addresses (e.g., \"user@example.com\" â†’ <EMAIL>). <PHONE>: For phone numbers (e.g., \"(123) 456-7890\" â†’ <PHONE>). <CURRENCY>: For monetary values (e.g., \"10.5\" â†’ <CURRENCY>). <NAME>: For proper names (e.g., \"John Smith\" â†’ <NAME>). <LOCATION>: For geographic locations (e.g., \"New York City\" â†’ <LOCATION>). <ORGANIZATION>: For organizations or companies (e.g., \"Google\" â†’ <ORGANIZATION>). <PRODUCT>: For product names (e.g., \"iPhone 14\" â†’ <PRODUCT>). Content-Specific Masks <HASHTAG>: For hashtags in social media (e.g., \"#LoveCoding\" â†’ <HASHTAG>). <MENTION>: For social media mentions (e.g., \"@user123\" â†’ <MENTION>). <TOKEN>: For general tokens that don't fit into other categories. <SENSITIVE>: For sensitive information like ID numbers, passwords, or personal data. <PLATE>: For license plates or other standardized codes. <FILE>: For file paths or filenames (e.g., \"/path/to/file.txt\" â†’ <FILE>). Custom Masks Depending on the application, you can create custom masks for domain-specific data: <DISEASE>: For medical terms (e.g., \"diabetes\" â†’ <DISEASE>). <SYMPTOM>: For symptoms in clinical text (e.g., \"fever\" â†’ <SYMPTOM>). <CHEMICAL>: For chemical names (e.g., \"H2SO4\" â†’ <CHEMICAL>). <TAG>: For labels in annotated datasets (e.g., <POSITIVE> for sentiment analysis). Miscellaneous <MASK>: A general mask for masked language models like BERT. <UNKNOWN> or <UNK>: For unknown or out-of-vocabulary tokens. <DIGIT>: If distinguishing single digits from other numbers is important. <AGE>: For age values (e.g., \"25 years old\" â†’ <AGE>). Application-Specific Examples For legal, medical, or technical fields, masks can be tailored. For example: In legal documents: <CASENUMBER>, <LAWSECTION>. In medical records: <PATIENTID>, <DIAGNOSIS>. In technical logs: <IPADDRESS>, <ERROR_CODE>. Would you like guidance on implementing a specific set of masks for your use case?","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Preprocessing_Pipeline_JuML","category":"page"},{"location":"#Preprocessing*Pipeline*JuML","page":"Home","title":"PreprocessingPipelineJuML","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Coverage) (Image: Build Status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"PreprocessingPipelineJuML is a Julia package for preprocessing text data in NLP pipelines.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Text preprocessing: prepare textual data for machine learning tasks. Preprocessing steps include:\napplied before Tokenization:\nexpansion of contractions\nmasking of numbers\nnoise removal (punctuation, special characters, phone numbers, e-mail addresses, ...)\ntext standardization (handle encoding & lowercasing)\napplied after Tokenization:\nstopword removal\nstemming\nTokenization: Split text into words or characters.\nVectorization: Transform text into machine-learning-compatible vector representations\none-hot encoding\nBag of Words (BoW)\nBag of N-Grams\nTerm Frequency-Inverse Document Frequency (TF-IDF)","category":"page"},{"location":"#API-Structure","page":"Home","title":"API Structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package provides a set of pipeline stages that can be chained together to preprocess text data. The pipeline stages are implemented as functions that take a NlpPipe or TokenizedNlpPipe struct as input and return a modified object of the same type. This makes it easy to build custom preprocessing pipelines by piping together the desired stages.","category":"page"},{"location":"#Usage-Example:","page":"Home","title":"Usage Example:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"corpus = [\"Hello, world!\", \"How are you?\"]\npipe = NlpPipe(corpus) |> remove_noise |> tokenize |> one_hot_encoding\n# Output: VectorizedNlpPipe with one-hot encoded tokens","category":"page"},{"location":"#Pipe-Objects:","page":"Home","title":"Pipe Objects:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"NlpPipe First struct to instantiate in a pipeline. Can be created directly from a text corpus. Can be ","category":"page"},{"location":"","page":"Home","title":"Home","text":"used in preprocessing stages that do not require the text to be tokenized.\ntransformed into a TokenizedNlpPipe by applying the tokenize function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"TokenizedNlpPipe Struct that holds tokenized text data. Can be used for preprocessing stages that require tokenized text (e.g., stopword removal, stemming, etc.). Can be transformed into a VectorizedNlpPipe by applying any vectorization function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"VectorizedNlpPipe Struct that holds vectorized text data (embeddings). Can be used for machine learning tasks.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Pipeline Diagram)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [Preprocessing_Pipeline_JuML]","category":"page"},{"location":"#Preprocessing_Pipeline_JuML.NlpPipe","page":"Home","title":"Preprocessing_Pipeline_JuML.NlpPipe","text":"NlpPipe\n\nA simple pipeline structure for handling text data (corpus) and corresponding labels.\n\nFields\n\ncorpus::Vector{String}: A collection of text documents.\nlabels::Union{Vector{String}, Nothing}: Optional labels corresponding to each document in corpus.\n\nConstructors\n\nNlpPipe(corpus::Vector{String}, labels::Union{Vector{String}, Nothing})    Creates an NlpPipe instance with a given corpus and optional labels.    Throws an ArgumentError if the number of documents and labels do not match.\nNlpPipe(corpus::Vector{String})    Creates an NlpPipe instance with only a corpus, setting labels to nothing.\nNlpPipe(corpus::String)    Creates an NlpPipe instance with a single document, storing it in a vector.\nNlpPipe(previousPipe::NlpPipe; corpus::Vector{String} = previousPipe.corpus, labels::Union{Vector{String}, Nothing} = previousPipe.labels)    Creates a new NlpPipe instance based on an existing one, optionally overriding corpus and labels.    Throws an ArgumentError if labels is not nothing and its length does not match the corpus length.\n\nExample Usage\n\nCreating a pipe from a corpus with multiple documents, inclusing labels\n\njulia> pipe1 = NlpPipe([\"document1\", \"document2\"], [\"label1\", \"label2\"])\nNlpPipe([\"document1\", \"document2\"], [\"label1\", \"label2\"])\n\n\n\nCreating a pipe from a corpus without labels\n\njulia> pipe2 = NlpPipe([\"document3\"])\nNlpPipe([\"document3\"], nothing)\n\n\n\nCreating a pipe from a single string corpus\n\njulia> pipe3 = NlpPipe(\"single document\") \nNlpPipe([\"single document\"], nothing)\n\n\n\nCreating a new pipe from an existing one with modified corpus and labels\n\njulia> NlpPipe(pipe1, corpus=[\"new_doc1\", \"new_doc2\"])\nNlpPipe([\"new_doc1\", \"new_doc2\"], [\"label1\", \"label2\"])\n\n\n\n\n\n\n\n","category":"type"},{"location":"#Preprocessing_Pipeline_JuML.TokenizedNlpPipe","page":"Home","title":"Preprocessing_Pipeline_JuML.TokenizedNlpPipe","text":"TokenizedNlpPipe\n\nA structure for handling tokenized text data, maintaining a vocabulary and optional labels.\n\nFields\n\ncorpus::Vector{String}: A collection of original text documents.\ntokens::Vector{Vector{String}}: Tokenized representation of each document in corpus.\nvocabulary::Set{String}: A set of unique tokens derived from tokens.\nlabels::Union{Vector{String}, Nothing}: Optional labels corresponding to each document.\n\nConstructors\n\nTokenizedNlpPipe(corpus::Vector{String}, tokens::Vector{Vector{String}}, labels::Union{Vector{String}, Nothing})    Creates a TokenizedNlpPipe instance with a given corpus, tokenized documents, and optional labels.    The vocabulary is automatically generated from tokens.\nTokenizedNlpPipe(previousPipe::TokenizedNlpPipe; tokens::Vector{Vector{String}} = previousPipe.tokens, vocabulary::Set{String} = previousPipe.vocabulary, labels::Union{Vector{String}, Nothing} = previousPipe.labels)    Creates a new TokenizedNlpPipe instance based on an existing one, allowing modifications to tokens, vocabulary, and labels while retaining the original corpus.\n\nExample Usage\n\n\n\nCreating a pipe from an NlpPipe instance (usual way to do it)\n\njulia> corpus = [\"Hello world\", \"Julia is great\"]\n2-element Vector{String}:\n \"Hello world\"\n \"Julia is great\"\njulia> tokenizedPipe = NlpPipe(corpus) |> tokenize\nTokenizedNlpPipe([\"Hello world\", \"Julia is great\"], [[\"Hello\", \"world\"], [\"Julia\", \"is\", \"great\"]], Set([\"great\", \"Hello\", \"is\", \"Julia\", \"world\"]), nothing)\n\n\n\nCreating a new pipe from scratch\n\njulia> corpus = [\"Hello world\", \"Julia is great\"]\n2-element Vector{String}:\n \"Hello world\"\n \"Julia is great\"\n\njulia> tokens = [[\"Hello\", \"world\"], [\"Julia\", \"is\", \"great\"]]\n2-element Vector{Vector{String}}:\n [\"Hello\", \"world\"]\n [\"Julia\", \"is\", \"great\"]\n \njulia> TokenizedNlpPipe(corpus, tokens, [\"greeting\", \"statement\"])\nTokenizedNlpPipe([\"Hello world\", \"Julia is great\"], [[\"Hello\", \"world\"], [\"Julia\", \"is\", \"great\"]], Set([\"great\", \"Hello\", \"is\", \"Julia\", \"world\"]), [\"greeting\", \"statement\"])\n\n\n\nCreating a new pipe from an existing one with modified tokens\n\njulia> pipe1 = TokenizedNlpPipe(corpus, tokens, [\"greeting\", \"statement\"])\nTokenizedNlpPipe([\"Hello world\", \"Julia is great\"], [[\"Hello\", \"world\"], [\"Julia\", \"is\", \"great\"]], Set([\"great\", \"Hello\", \"is\", \"Julia\", \"world\"]), [\"greeting\", \"statement\"])\n\njulia> pipe2 = TokenizedNlpPipe(pipe1; tokens=[[\"Hello\"], [\"Julia\", \"is\"]])\nTokenizedNlpPipe([\"Hello world\", \"Julia is great\"], [[\"Hello\"], [\"Julia\", \"is\"]], Set([\"great\", \"Hello\", \"is\", \"Julia\", \"world\"]), [\"greeting\", \"statement\"])\n\n\n\n\n\n\n\n","category":"type"},{"location":"#Preprocessing_Pipeline_JuML.VectorizedNlpPipe","page":"Home","title":"Preprocessing_Pipeline_JuML.VectorizedNlpPipe","text":"VectorizedNlpPipe\n\nA structure for handling vectorized representations of tokenized text data, including a vocabulary mapping and optional labels.\n\nFields\n\ntokens::Vector{Matrix{<:Union{Int, Float64}}}: A collection of numerical representations (e.g., embeddings, one-hot encodings) for tokenized text.\nvocabulary::Dict{String, Int}: A dictionary mapping words to unique integer indices.\nlabels::Union{Vector{String}, Nothing}: Optional labels corresponding to each document.\n\nExample Usage\n\n\n\nCreating a pipe from an existing TokenizedNlpPipe instance (usual way to do it)\n\njulia> corpus = [\"Hello world\", \"Julia is great\"]\n2-element Vector{String}:\n \"Hello world\"\n \"Julia is great\"\n\njulia> NlpPipe(corpus) |> tokenize |> one_hot_encoding # (or any other vectorization method)\nVectorizedNlpPipe(Matrix{<:Union{Float64, Int64}}[[0 1 â€¦ 0 0; 0 0 â€¦ 0 1], [0 0 â€¦ 1 0; 0 0 â€¦ 0 0; 1 0 â€¦ 0 0]], Dict(\"great\" => 1, \"Hello\" => 2, \"is\" => 3, \"Julia\" => 4, \"world\" => 5), nothing)\n\n\n\nCreating a pipe from scratch\n\njulia> tokens = [[1 2; 3 4], [5 6; 7 8]]  # Example word embeddings (each document is a matrix)\n2-element Vector{Matrix{Int64}}:\n [1 2; 3 4]\n [5 6; 7 8]\n\njulia> vocab = Dict(\"hello\" => 1, \"world\" => 2, \"Julia\" => 3)\nDict{String, Int64} with 3 entries:\n  \"hello\" => 1\n  \"Julia\" => 3\n  \"world\" => 2\n\njulia> labels = [\"greeting\", \"statement\"]\n2-element Vector{String}:\n \"greeting\"\n \"statement\"\n\njulia> VectorizedNlpPipe(tokens, vocab, labels)\nVectorizedNlpPipe(Matrix{<:Union{Float64, Int64}}[[1 2; 3 4], [5 6; 7 8]], Dict(\"hello\" => 1, \"Julia\" => 3, \"world\" => 2), [\"greeting\", \"statement\"])\n\n\n\n\n\n\n\n","category":"type"},{"location":"#Preprocessing_Pipeline_JuML.bag_of_ngrams-Tuple{TokenizedNlpPipe}","page":"Home","title":"Preprocessing_Pipeline_JuML.bag_of_ngrams","text":"bag_of_ngrams(pipe::TokenizedNlpPipe; n::Int = 1) -> VectorizedNlpPipe\n\nCreate a bag of n-grams out of given TokenizedNlpPipe, with padding for shorter documents.\n\nArguments:\n\npipe::TokenizedNlpPipe: The input TokenizedNlpPipe object containing the tokenized documents.\nn::Int: The n-gram size. Defaults to 1.\n\nReturns:\n\nVectorizedNlpPipe: A new VectorizedNlpPipe object with the n-gram vectors.\n\nUsage Example:\n\njulia> NlpPipe([\"words one\", \"words two\"]) |> tokenize |> bag_of_ngrams\nVectorizedNlpPipe(Matrix{<:Union{Float64, Int64}}[[1 0 0; 0 1 0], [1 0 0; 0 0 1]], Dict(\"two\" => 3, \"one\" => 2, \"words\" => 1), nothing)\n\n\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.bag_of_words-Tuple{TokenizedNlpPipe}","page":"Home","title":"Preprocessing_Pipeline_JuML.bag_of_words","text":"bag_of_words(pipe::TokenizedNlpPipe) -> VectorizedNlpPipe\n\nCreate a bag-of-words-encoding out of given TokenizedNlpPipe\n\nArguments:\n\npipe::TokenizedNlpPipe: The input TokenizedNlpPipe object containing the tokenized documents.\n\nReturns:\n\nVectorizedNlpPipe: A new VectorizedNlpPipe object with the bag-of-words vectors.\n\nExamples:\n\njulia> NlpPipe([\"words one\", \"words two\"]) |> tokenize |> bag_of_words\nVectorizedNlpPipe(Matrix{<:Union{Float64, Int64}}[[0 1 1], [1 0 1]], Dict(\"two\" => 1, \"one\" => 2, \"words\" => 3), nothing)\n\n\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.create_n_gram_dict-Tuple{TokenizedNlpPipe, Int64}","page":"Home","title":"Preprocessing_Pipeline_JuML.create_n_gram_dict","text":"create_n_gram_dict(pipe, n)\n\nCreate a dictionary of ngrams out of given TokenizedNlpPipe.\n\nExamples:\n\njulia> pipe = TokenizedNlpPipe([[\"one\", \"sentence\", \"sample\"],[\"two\", \"sentence\", \"sample\"]])\njulia> create_n_gram_dict(pipe, 2)\nDict(\"two sentence\" => 2, \"sentence sample\" => 3, \"one sentence\" => 1)\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.expand_contractions-Tuple{NlpPipe}","page":"Home","title":"Preprocessing_Pipeline_JuML.expand_contractions","text":"expand_contractions(input::NlpPipe) -> NlpPipe\n\nExpand contractions in the input text. This function expands common English contractions.\n\nArguments\n\ninput::NlpPipe: A NlpPipe object containing the corpus to expand contractions in.\n\nReturns\n\noutput::NlpPipe: A new NlpPipe object with the contractions expanded in the corpus.\n\nUsage Example\n\njulia> NlpPipe([\"I'm happy\", \"I've got a cat\"]) |> expand_contractions\nNlpPipe([\"I am happy\", \"I have got a cat\"], nothing)\n\n\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.mask_numbers-Tuple{NlpPipe}","page":"Home","title":"Preprocessing_Pipeline_JuML.mask_numbers","text":"mask_numbers(pipe::NlpPipe; replace_with::String=\"<NUM>\") -> NlpPipe\n\nReplaces all numbers in the text of the given NlpPipe corpus with a specified string.\n\nArguments\n\npipe::NlpPipe: The input NlpPipe object containing the corpus to be processed.\nreplace_with::String: The string to replace numbers with. Defaults to \"<NUM>\".\n\nReturns\n\nNlpPipe: A new NlpPipe object with the numbers in the corpus replaced by the specified string.\n\nExample\n\njulia> NlpPipe([\"The price is 1000â‚¬.\"]) |> mask_numbers\nNlpPipe([\"The price is <NUM>â‚¬.\"], nothing)\n\n\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.one_hot_encoding-Tuple{TokenizedNlpPipe}","page":"Home","title":"Preprocessing_Pipeline_JuML.one_hot_encoding","text":"one_hot_encoding(pipe::TokenizedNlpPipe) -> VectorizedNlpPipe\n\nCreate a one-hot-encoding out of given TokenizedNlpPipe\n\nArguments:\n\npipe::TokenizedNlpPipe: The input TokenizedNlpPipe object containing the tokenized documents.\n\nReturns:\n\nVectorizedNlpPipe: The output VectorizedNlpPipe object containing the one-hot-encoded documents.\n\nExamples:\n\njulia> NlpPipe([\"words one\", \"words two\"]) |> tokenize |> one_hot_encoding\nVectorizedNlpPipe(Matrix{<:Union{Float64, Int64}}[[0 0 1; 0 1 0], [0 0 1; 1 0 0]], Dict(\"two\" => 1, \"one\" => 2, \"words\" => 3), nothing)\n\n\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.remove_noise-Tuple{NlpPipe}","page":"Home","title":"Preprocessing_Pipeline_JuML.remove_noise","text":"remove_noise(pipe::NlpPipe) -> NlpPipe\n\nRemoves noise from the corpus. Noise includes HTML tags, URLs, email addresses, file paths, special characters, dates & times.\n\nArguments\n\npipe::NlpPipe: The NlpPipe object with a corpus to remove noise from\n\nReturns\n\nA new NlpPipe object with the noise removed from the corpus\n\nUsage Examples\n\njulia> NlpPipe([\"<html>This is a test</html>\"]) |> remove_noise\nNlpPipe([\"This is a test\"], nothing)\n\n\n\nWith custom replacement patterns\n\njulia> NlpPipe([\"<html>This is a test</html>\"]) |> pipe -> remove_noise(pipe, replacement_patterns=[r\"is a\" => \"ðŸ¦–ðŸ«¶\"])\nNlpPipe([\"<html>This ðŸ¦–ðŸ«¶ test</html>\"], nothing)\n\n\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.remove_stop_words-Tuple{TokenizedNlpPipe}","page":"Home","title":"Preprocessing_Pipeline_JuML.remove_stop_words","text":"remove_stop_words(pipe::TokenizedNlpPipe; language::String=\"en\", stop_words::Set{String}=Set{String}()) -> TokenizedNlpPipe\n\nRemoves predefined stopwords. You can access the stop words for a given language using the language name or ISO 639 code.  For example, to get the stop words for English, you can use stopwords[\"eng\"], stopwords[\"en\"], or stopwords[\"English\"].\n\nStop words sourced from https://github.com/guo-yong-zhi/StopWords.jl/blob/main/README.md.\n\nParameters\n\npipe: TokenizedNlpPipe\nlanguage: String = \"en\"\nstop_words: Set{String} = Set{String}()\n\nReturns\n\nA new TokenizedNlpPipestruct with the stop words removed from the tokens.\n\nExamples\n\nRemoving stop words from a tokenized pipe (default stop words)\n\njulia> NlpPipe([\"This is a dinosaur\"]) |> tokenize |> remove_stop_words |> pipe -> pipe.tokens\n1-element Vector{Vector{String}}:\n [\"This\", \"dinosaur\"]\n\n\n\nUsing custom stop words\n\njulia> NlpPipe([\"This is a dinosaur\"]) |> tokenize |> pipe -> remove_stop_words(pipe, stop_words=Set([\"This\", \"dinosaur\"])) |> pipe -> pipe.tokens\n1-element Vector{Vector{String}}:\n [\"is\", \"a\"]\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.standardize_document-Tuple{String}","page":"Home","title":"Preprocessing_Pipeline_JuML.standardize_document","text":"standardize_document(doc::String)::String\n\nStandardizes a document by converting it to lowercase and replacing unusual characters with their standard counterparts.\n\nArguments\n\ndoc::String: The input document as a string.\n\nReturns\n\nString: The standardized document as a string.\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.standardize_text-Tuple{NlpPipe}","page":"Home","title":"Preprocessing_Pipeline_JuML.standardize_text","text":"standardize_text(pipe::NlpPipe) -> NlpPipe\n\nApplies the standardize_document function to each document in the corpus of the given NlpPipe object.\n\nArguments\n\npipe::NlpPipe: An NlpPipe object containing a corpus and associated labels.\n\nReturns\n\nNlpPipe: A new NlpPipe object with the standardized corpus and the original labels.\n\nUsage Example\n\njulia> NlpPipe([\"Hello WORLD\", \"Julia is GREAT\"]) |> standardize_text\nNlpPipe([\"hello world\", \"julia is great\"], nothing)\n\n\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.tf_idf-Tuple{TokenizedNlpPipe}","page":"Home","title":"Preprocessing_Pipeline_JuML.tf_idf","text":"tf_idf(pipe::TokenizedNlpPipe; tf_weighting::String = \"relative term frequency\", idf_weighting::String=\"inverse document frequency\") -> VectorizedNlpPipe\n\nCompute the TF-IDF (Term Frequency-Inverse Document Frequency) representation of the tokenized documents in the given pipe.\n\nArguments\n\npipe::TokenizedNlpPipe: A pipeline containing tokenized documents.\ntf_weighting::String: The term frequency weighting scheme. Options are \"relative term frequency\" (default) and \"raw term frequency\".\nidf_weighting::String: The inverse document frequency weighting scheme. Options are \"inverse document frequency\" (default) and \"smooth inverse document frequency\".\n\nReturns\n\nVectorizedNlpPipe: A new pipeline containing the TF-IDF vectorized representation of the documents.\n\nUsage Examples\n\njulia> NlpPipe([\"words one\", \"words two\"]) |> tokenize |> tf_idf\nVectorizedNlpPipe(Matrix{<:Union{Float64, Int64}}[[0.0 0.0 0.0; 0.0 0.35 0.0], [0.0 0.0 0.0; 0.0 0.0 0.35]], Dict(\"two\" => 3, \"one\" => 2, \"words\" => 1), nothing)\n\n\n\n\n\n\n\n","category":"method"},{"location":"#Preprocessing_Pipeline_JuML.tokenize","page":"Home","title":"Preprocessing_Pipeline_JuML.tokenize","text":"tokenize(pipe::NlpPipe, level::Symbol = :word) -> TokenizedNlpPipe\n\nTokenizes the documents in the corpus of the given NlpPipe object. The level parameter can be set to :word or :character.\n\nArguments\n\npipe::NlpPipe: An NlpPipe object containing a corpus of documents.\nlevel::Symbol: The tokenization level, either :word (default) or :character.\n\nReturns\n\nTokenizedNlpPipe: A new TokenizedNlpPipe object with the tokenized documents.\n\nUsage Example\n\njulia> NlpPipe([\"Hello world\", \"Julia is great\"]) |> tokenize\nTokenizedNlpPipe([\"Hello world\", \"Julia is great\"], [[\"Hello\", \"world\"], [\"Julia\", \"is\", \"great\"]], Set([\"great\", \"Hello\", \"is\", \"Julia\", \"world\"]), nothing)\n\n\n\n\n\n\n\n","category":"function"}]
}
